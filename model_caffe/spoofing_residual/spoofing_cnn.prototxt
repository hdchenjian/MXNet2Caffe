name: "LeNet"

layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape: { dim: 1 dim: 3 dim: 112 dim: 96 }
  }
}

layer {
	bottom: "data"
	top: "conv0"
	name: "conv0"
	type: "Convolution"
	convolution_param {
		num_output: 64
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "conv0"
  top: "relu0"
  name: "relu0"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "relu0"
	top: "stage1_unit1_conv1"
	name: "stage1_unit1_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 64
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage1_unit1_conv1"
  top: "stage1_unit1_relu1"
  name: "stage1_unit1_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage1_unit1_relu1"
	top: "stage1_unit1_conv2"
	name: "stage1_unit1_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 64
		kernel_size: 3
		pad: 1
		stride: 2
		bias_term: true
	}
}


layer {
  bottom: "stage1_unit1_conv2"
  top: "stage1_unit1_relu2"
  name: "stage1_unit1_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "relu0"
	top: "stage1_unit1_conv1sc"
	name: "stage1_unit1_conv1sc"
	type: "Convolution"
	convolution_param {
		num_output: 64
		kernel_size: 1
		stride: 2
		bias_term: true
	}
}

layer {
  bottom: "stage1_unit1_conv1sc"
  top: "stage1_unit1_relu3"
  name: "stage1_unit1_relu3"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus0"
  type: "Eltwise"
  bottom: "stage1_unit1_relu2"
  bottom: "stage1_unit1_relu3"
  top: "_plus0"
  eltwise_param { operation: SUM }
}


layer {
	bottom: "_plus0"
	top: "stage2_unit1_conv1"
	name: "stage2_unit1_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 64
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage2_unit1_conv1"
  top: "stage2_unit1_relu1"
  name: "stage2_unit1_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage2_unit1_relu1"
	top: "stage2_unit1_conv2"
	name: "stage2_unit1_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 64
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage2_unit1_conv2"
  top: "stage2_unit1_relu2"
  name: "stage2_unit1_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus1"
  type: "Eltwise"
  bottom: "_plus0"
  bottom: "stage2_unit1_relu2"
  top: "_plus1"
  eltwise_param { operation: SUM }
}

layer {
	bottom: "_plus1"
	top: "stage2_unit2_conv1"
	name: "stage2_unit2_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 64
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage2_unit2_conv1"
  top: "stage2_unit2_relu1"
  name: "stage2_unit2_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage2_unit2_relu1"
	top: "stage2_unit2_conv2"
	name: "stage2_unit2_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 64
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage2_unit2_conv2"
  top: "stage2_unit2_relu2"
  name: "stage2_unit2_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus2"
  type: "Eltwise"
  bottom: "_plus1"
  bottom: "stage2_unit2_relu2"
  top: "_plus2"
  eltwise_param { operation: SUM }
}

layer {
	bottom: "_plus2"
	top: "stage2_unit3_conv1"
	name: "stage2_unit3_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 128
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage2_unit3_conv1"
  top: "stage2_unit3_relu1"
  name: "stage2_unit3_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage2_unit3_relu1"
	top: "stage2_unit3_conv2"
	name: "stage2_unit3_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 128
		kernel_size: 3
		pad: 1
		stride: 2
		bias_term: true
	}
}

layer {
  bottom: "stage2_unit3_conv2"
  top: "stage2_unit3_relu2"
  name: "stage2_unit3_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "_plus2"
	top: "stage2_unit3_conv1sc"
	name: "stage2_unit3_conv1sc"
	type: "Convolution"
	convolution_param {
		num_output: 128
		kernel_size: 1
		stride: 2
		bias_term: true
	}
}

layer {
  bottom: "stage2_unit3_conv1sc"
  top: "stage2_unit3_relu3"
  name: "stage2_unit3_relu3"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus3"
  type: "Eltwise"
  bottom: "stage2_unit3_relu2"
  bottom: "stage2_unit3_relu3"
  top: "_plus3"
  eltwise_param { operation: SUM }
}



layer {
	bottom: "_plus3"
	top: "stage3_unit1_conv1"
	name: "stage3_unit1_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 128
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage3_unit1_conv1"
  top: "stage3_unit1_relu1"
  name: "stage3_unit1_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage3_unit1_relu1"
	top: "stage3_unit1_conv2"
	name: "stage3_unit1_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 128
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage3_unit1_conv2"
  top: "stage3_unit1_relu2"
  name: "stage3_unit1_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus4"
  type: "Eltwise"
  bottom: "_plus3"
  bottom: "stage3_unit1_relu2"
  top: "_plus4"
  eltwise_param { operation: SUM }
}

layer {
	bottom: "_plus4"
	top: "stage3_unit2_conv1"
	name: "stage3_unit2_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 128
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage3_unit2_conv1"
  top: "stage3_unit2_relu1"
  name: "stage3_unit2_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage3_unit2_relu1"
	top: "stage3_unit2_conv2"
	name: "stage3_unit2_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 128
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage3_unit2_conv2"
  top: "stage3_unit2_relu2"
  name: "stage3_unit2_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus5"
  type: "Eltwise"
  bottom: "_plus4"
  bottom: "stage3_unit2_relu2"
  top: "_plus5"
  eltwise_param { operation: SUM }
}

layer {
	bottom: "_plus5"
	top: "stage3_unit3_conv1"
	name: "stage3_unit3_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 256
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage3_unit3_conv1"
  top: "stage3_unit3_relu1"
  name: "stage3_unit3_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage3_unit3_relu1"
	top: "stage3_unit3_conv2"
	name: "stage3_unit3_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 256
		kernel_size: 3
		pad: 1
		stride: 2
		bias_term: true
	}
}

layer {
  bottom: "stage3_unit3_conv2"
  top: "stage3_unit3_relu2"
  name: "stage3_unit3_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "_plus5"
	top: "stage3_unit3_conv1sc"
	name: "stage3_unit3_conv1sc"
	type: "Convolution"
	convolution_param {
		num_output: 256
		kernel_size: 1
		stride: 2
		bias_term: true
	}
}

layer {
  bottom: "stage3_unit3_conv1sc"
  top: "stage3_unit3_relu3"
  name: "stage3_unit3_relu3"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus6"
  type: "Eltwise"
  bottom: "stage3_unit3_relu2"
  bottom: "stage3_unit3_relu3"
  top: "_plus6"
  eltwise_param { operation: SUM }
}

layer {
	bottom: "_plus6"
	top: "stage4_unit1_conv1"
	name: "stage4_unit1_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 256
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage4_unit1_conv1"
  top: "stage4_unit1_relu1"
  name: "stage4_unit1_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage4_unit1_relu1"
	top: "stage4_unit1_conv2"
	name: "stage4_unit1_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 256
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage4_unit1_conv2"
  top: "stage4_unit1_relu2"
  name: "stage4_unit1_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus7"
  type: "Eltwise"
  bottom: "_plus6"
  bottom: "stage4_unit1_relu2"
  top: "_plus7"
  eltwise_param { operation: SUM }
}

layer {
	bottom: "_plus7"
	top: "stage4_unit2_conv1"
	name: "stage4_unit2_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 256
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage4_unit2_conv1"
  top: "stage4_unit2_relu1"
  name: "stage4_unit2_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage4_unit2_relu1"
	top: "stage4_unit2_conv2"
	name: "stage4_unit2_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 256
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage4_unit2_conv2"
  top: "stage4_unit2_relu2"
  name: "stage4_unit2_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus8"
  type: "Eltwise"
  bottom: "_plus7"
  bottom: "stage4_unit2_relu2"
  top: "_plus8"
  eltwise_param { operation: SUM }
}

layer {
	bottom: "_plus8"
	top: "stage4_unit3_conv1"
	name: "stage4_unit3_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 512
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage4_unit3_conv1"
  top: "stage4_unit3_relu1"
  name: "stage4_unit3_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage4_unit3_relu1"
	top: "stage4_unit3_conv2"
	name: "stage4_unit3_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 512
		kernel_size: 3
		pad: 1
		stride: 2
		bias_term: true
	}
}

layer {
  bottom: "stage4_unit3_conv2"
  top: "stage4_unit3_relu2"
  name: "stage4_unit3_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "_plus8"
	top: "stage4_unit3_conv1sc"
	name: "stage4_unit3_conv1sc"
	type: "Convolution"
	convolution_param {
		num_output: 512
		kernel_size: 1
		stride: 2
		bias_term: true
	}
}

layer {
  bottom: "stage4_unit3_conv1sc"
  top: "stage4_unit3_relu3"
  name: "stage4_unit3_relu3"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus9"
  type: "Eltwise"
  bottom: "stage4_unit3_relu2"
  bottom: "stage4_unit3_relu3"
  top: "_plus9"
  eltwise_param { operation: SUM }
}


layer {
	bottom: "_plus9"
	top: "stage5_unit1_conv1"
	name: "stage5_unit1_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 512
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage5_unit1_conv1"
  top: "stage5_unit1_relu1"
  name: "stage5_unit1_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage5_unit1_relu1"
	top: "stage5_unit1_conv2"
	name: "stage5_unit1_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 512
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage5_unit1_conv2"
  top: "stage5_unit1_relu2"
  name: "stage5_unit1_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus10"
  type: "Eltwise"
  bottom: "_plus9"
  bottom: "stage5_unit1_relu2"
  top: "_plus10"
  eltwise_param { operation: SUM }
}

layer {
	bottom: "_plus10"
	top: "stage5_unit2_conv1"
	name: "stage5_unit2_conv1"
	type: "Convolution"
	convolution_param {
		num_output: 512
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage5_unit2_conv1"
  top: "stage5_unit2_relu1"
  name: "stage5_unit2_relu1"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
	bottom: "stage5_unit2_relu1"
	top: "stage5_unit2_conv2"
	name: "stage5_unit2_conv2"
	type: "Convolution"
	convolution_param {
		num_output: 512
		kernel_size: 3
		pad: 1
		stride: 1
		bias_term: true
	}
}

layer {
  bottom: "stage5_unit2_conv2"
  top: "stage5_unit2_relu2"
  name: "stage5_unit2_relu2"
  type: "PReLU"
  relu_param{
    negative_slope: 0.1
  }
}

layer {
  name: "_plus11"
  type: "Eltwise"
  bottom: "_plus10"
  bottom: "stage5_unit2_relu2"
  top: "_plus11"
  eltwise_param { operation: SUM }
}

layer {
  name: "pool1"
  type: "Pooling"
  bottom: "_plus11"
  top: "pool1"
  pooling_param {
    pool: AVE
    kernel_h: 7
    kernel_w: 6
    stride: 1
  }
}


layer {
	bottom: "pool1"
	top: "conv00"
	name: "conv00"
	type: "Convolution"
	convolution_param {
		num_output: 2
		kernel_size: 1
		pad: 0
		stride: 1
		bias_term: true
	}
}
